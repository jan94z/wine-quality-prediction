# This just tests if the db, the mlflow server, the api is working IN THEORY
# bc everything just runs locally, github actions can't access the existing containers, it just creates its own
# including model training just works because the model and the data is so small 

name: Test API with PostgreSQL and Docker Compose

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r shared/requirements.txt
          pip install -r ci/tests/requirements.txt

      - name: Set up Docker Compose
        run: sudo apt-get update && sudo apt-get install -y docker-compose

      - name: Start Docker Compose services
        run: | 
            cd ci
            docker compose up --build -d db
            docker compose up --build -d db-init
            docker compose up --build -d mlflow
            docker compose up --build -d training
            docker compose up --build -d promotion
            docker compose run --rm promotion --alias Production
            docker compose up --build -d api
            cd ..

      - name: Wait for API to be ready
        run: |
          for i in {1..10}; do
            curl -s http://localhost:8000/docs && break
            echo "Waiting for API..."
            sleep 3
          done

      - name: Run tests
        run: PYTHONPATH=$(pwd) pytest ci/tests/tests.py

      - name: Shutdown services
        run: docker compose -f ci/docker-compose.yml down

